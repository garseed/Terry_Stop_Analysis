{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terry Stop Analysis\n",
    "### In the 1968 Supreme Court case \"Terry v. Ohio\", the court found that a police officer was not in vilation of the \"unresonable search and seizure\" clause of the Fourth Amendment after he stopped and frisked suspects only because their behavior was suspicious. Thus the phrase \"Terry Stops\" are in reference to stops made of suspicious drivers. \n",
    "\n",
    "This is an analysis of over 48,000 Terry Stops, with a goal of predicting if an arrest will be made based off time of day, whether a suspect was frisked, and racial & gender demographics of both the suspects and officers. \n",
    "\n",
    "The overall goal of the analysis is to have the highest possible recall, to minimize false positives, of accidentally classifying subject who were not arrested as arrested. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries. \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'Terry_Stops.csv' does not exist: b'Terry_Stops.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9fed5fea44e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display.max_columns'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Terry_Stops.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Garseed/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Garseed/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Garseed/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Garseed/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Garseed/anaconda3/envs/learn-env/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'Terry_Stops.csv' does not exist: b'Terry_Stops.csv'"
     ]
    }
   ],
   "source": [
    "# Import and look at the rows of our dataset. \n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "df = pd.read_csv('Terry_Stops.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For ease of analysis, let's add underscores to each column name\n",
    "renamed_columns = ['Subject_Age_Group','Subject_ID', 'GO_SC_Num', 'Terry_Stop_ID',\n",
    "                   'Stop_Resolution', 'Weapon_Type', 'Officer_ID', 'Officer_YOB',\n",
    "                  'Officer_Gender', 'Officer_Race', 'Subject_Perceived_Race',\n",
    "                  'Subject_Perceived_Gender', 'Reported_Date', 'Reported_Time',\n",
    "                   'Initial_Call_Type', 'Final_Call_Type', 'Call_Type', 'Officer_Squad',\n",
    "                  'Arrest_Flag', 'Frisk_Flag','Precinct', 'Sector', 'Beat']\n",
    "df.columns = renamed_columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the initial data, 'Subject_ID, GO_SC_Num', 'Terry_Stop_ID', and 'Officer_ID' are purely \n",
    "# individual indentifiers so it's okay to drop those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Subject_ID','GO_SC_Num', 'Terry_Stop_ID', 'Officer_ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Take a look to see if there are any null values in the data \n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Interesting, because upon initial analysis, there are empty entries \n",
    "# in 'Subject Age Group', all of the 'Call Type' columns\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate the columns that are mostly '-' to see if there is data \n",
    "# or if it's mostly null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Subject Age Group:' , '\\n', df['Subject_Age_Group'].value_counts(),'\\n')\n",
    "print('Initial Call Type:', '\\n', df['Initial_Call_Type'].value_counts(),'\\n')\n",
    "print('Final Call Type:', '\\n',  df['Final_Call_Type'].value_counts(), '\\n')\n",
    "print('Call Type:', \"\\n\", df['Call_Type'].value_counts(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From this it seems like all \"Call Type\" is mostly unnecessary and can be dropped. Also 'Initial_Call_Type'\n",
    "# and 'Final_Call_Type' are missing over 13k columns so it's okay to drop those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Call_Type','Initial_Call_Type', 'Final_Call_Type'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Subject_Age_Group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df = df[df.Subject_Age_Group != '-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Subject_Age_Group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Officer_YOB'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def officer_yob_decade(x):\n",
    "    if (x <= 1959):\n",
    "        return '1900-1960'\n",
    "    elif (x > 1959) and (x <= 1969):\n",
    "        return '1960s'\n",
    "    elif (x > 1969) and (x <= 1979):\n",
    "        return '1970s'\n",
    "    elif (x > 1979) and (x <= 1989):\n",
    "        return '1980s'\n",
    "    elif (x > 1989) and (x <= 1999):\n",
    "        return '1990s'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Officer_Age_By_Decade'] = df['Officer_YOB'].apply(officer_yob_decade)\n",
    "df['Officer_Age_By_Decade'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore \"Arrest Flag\" and \"Stop Resolution\n",
    "#a s they both have arrest data that will serve\n",
    "# as the target variable for this exploration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Arrest_Flag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Stop_Resolution'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It seems there is a discrepency in Arrest data between 'Stop Resolution'\n",
    "# and \"Arrest_Flag\". I am making the executive decision to base this \n",
    "# exploration around \"Stop Resolution\" as it reads as more thorough\n",
    "# in it's reporting of resoltion rather than a simple 'Yes'/'No' in \n",
    "# 'Arrest Flag'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Stop_Resolution'] = df['Stop_Resolution'].apply(lambda x: 'Yes' if x == 'Arrest' else 'No')\n",
    "df['Stop_Resolution'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Arrest_Flag', axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now explore factors like the races and genders of both the officers\n",
    "# and the people pulled over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Officer Gender:' , '\\n', df['Officer_Gender'].value_counts(),'\\n')\n",
    "print('Officer Race:', '\\n', df['Officer_Race'].value_counts(),'\\n')\n",
    "print('Subject Perceived Gender:', '\\n',  df['Subject_Perceived_Gender'].value_counts(), '\\n')\n",
    "print('Subject Perceived Race:', \"\\n\", df['Subject_Perceived_Race'].value_counts(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There seem to be many different ways to say \"unknown\" in these columns\n",
    "# Let's combine the redundant values together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Officer_Race'] = df['Officer_Race'].apply(lambda x: \"Other\" if x in ['Not Specified', 'Unknown'] else x)\n",
    "df['Subject_Perceived_Gender'] = df['Subject_Perceived_Gender'].apply(lambda x: 'Unknown/GNC' if x in ['Unable to Determine', '-', 'Unknown', 'Gender Diverse (gender non-conforming and/or transgender)'] else x)\n",
    "df['Subject_Perceived_Race'] = df['Subject_Perceived_Race'].apply(lambda x: 'Unknown' if x in ['-', 'Other'] else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['Officer_Race'].value_counts(), '\\n')\n",
    "print(df['Subject_Perceived_Gender'].value_counts(), '\\n')\n",
    "print(df['Subject_Perceived_Race'].value_counts(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also let's clean up the gender columns of both the officer and the \n",
    "# subject. Entries outside of the gender binary are miniscule compared\n",
    "# to Male and Female. \n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.Officer_Gender != 'N']\n",
    "df = df[df.Subject_Perceived_Gender != 'Unknown/GNC']\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Weapon_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Weapon_Type', axis=1)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Weapon type, there are far more null values than there are weapons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Reported_Time'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 13K different times, let's create a new column that is just \n",
    "# the hours of the stops. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Reported_Time'] = pd.to_datetime(df['Reported_Time'])\n",
    "df['Reported_Hour'] = df['Reported_Time'].apply(lambda x: x.hour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Reported_Hour'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_of_day(x):\n",
    "    if (x > 4) and (x <= 11):\n",
    "        return 'Morning'\n",
    "    elif (x > 12) and (x <= 19):\n",
    "        return 'Afternoon'\n",
    "    else:\n",
    "        return \"Night\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Time_of_Day'] = df['Reported_Hour'].apply(time_of_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Time_of_Day'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that those are divided, let's divide them into AM and PM and \n",
    "# create a new column. 0 is AM, 1 is PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Reported_AM_or_PM'] = df['Reported_Hour'].apply(lambda x: 0 if x <12 else 1)\n",
    "df['Reported_AM_or_PM'] = df['Reported_AM_or_PM'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Reported_AM_or_PM'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Reported_Time'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Frisk_Flag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.Frisk_Flag != '-']\n",
    "df['Frisk_Flag'] = df['Frisk_Flag'].apply(lambda x: \"0\" if x =='N' else '1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Frisk_Flag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Reported_Date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Reported_Date'] = pd.DatetimeIndex(df['Reported_Date']).month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Reported_Date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Precinct'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.Precinct != '-']\n",
    "df = df[df.Precinct != 'Unknown']\n",
    "df = df[df.Precinct != 'OOJ']\n",
    "df = df[df.Precinct != 'FK ERROR']\n",
    "df['Precinct'] = df['Precinct'].apply(lambda x: 'SouthWest' if x in ['Southwest'] else x)\n",
    "df['Precinct'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Beat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sector'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Officer_Squad'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping 'Officer Squad', 'Beat' and 'Sector' for reasons similar to Officer ID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(['Beat', 'Sector', 'Officer_Squad'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin visualization of cleaned data, starting with a visualizaiton of \n",
    "# our target variable, Stop Resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrests = df['Stop_Resolution']=='Yes' \n",
    "non_arrests = df['Stop_Resolution']=='No'\n",
    "y = df['Stop_Resolution']\n",
    "num_of_arrests = df[arrests].shape[0]\n",
    "num_of_nonarrests = df[non_arrests].shape[0]\n",
    "\n",
    "print('Target Variable: Stop Resolution')\n",
    "print('Total Arrests: ', num_of_arrests)\n",
    "print('Total of Non-arrests: ', num_of_nonarrests)\n",
    "\n",
    "sns.set_style('darkgrid');\n",
    "plt.figure(figsize = (10,5));\n",
    "sns.countplot(df['Stop_Resolution'], alpha = .80, palette= ['blue', 'red'])\n",
    "plt.title('Arrests & Non-Arrests');\n",
    "\n",
    "plt.ylabel('Number of people pulled over');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bar charts to compare different columns against the arrrest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_age_group = df.groupby(['Subject_Age_Group', 'Stop_Resolution']).Subject_Age_Group.count().unstack()\n",
    "subject_age_group['% of Stops Ending In Arrest'] = (subject_age_group['Yes'] / (subject_age_group.sum(axis=1)))*100 \n",
    "print('Subject Age Group\\n')\n",
    "print(subject_age_group)\n",
    "\n",
    "viz_1 = subject_age_group.plot(kind = 'bar', stacked = True,\n",
    "                           title = \"People Pulled Over By Age: Arrested v Not Arrested\",\n",
    "                           color = ['blue', 'red', 'white'], alpha = .70, rot=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_perceived_race = df.groupby(['Subject_Perceived_Race', 'Stop_Resolution']).Subject_Perceived_Race.count().unstack()\n",
    "subject_perceived_race['% of Stops Ending In Arrest'] = (subject_perceived_race['Yes'] / (subject_perceived_race.sum(axis=1)))*100 \n",
    "print('Subject Perceived Race\\n')\n",
    "print(subject_perceived_race)\n",
    "\n",
    "viz_2 = subject_perceived_race.plot(kind = 'bar', stacked = True,\n",
    "                                   title = 'People Pulled Over by Perceived Race:\\n Arrested v Not Arrested',\n",
    "                                   color = ['blue', 'red','white'], alpha = .80, rot=90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It appears American Inidan/Native Alaskan have the highest percentage\n",
    "# of arrests made after a Terry stop with nearly 37%. While important to note\n",
    "# there were only 669 total stops of 36363 total in the data set. \n",
    "# The next highest was Asian with 34.2% , with Black, white, and Native Hawaiian\n",
    "# at around 30% each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_perceived_gender = df.groupby(['Subject_Perceived_Gender', 'Stop_Resolution']).Subject_Perceived_Gender.count().unstack()\n",
    "subject_perceived_gender['% of Stops Ending In Arrest'] = (subject_perceived_gender['Yes'] / (subject_perceived_gender.sum(axis=1)))*100 \n",
    "\n",
    "\n",
    "print('Subject Perceived Gender\\n')\n",
    "print(subject_perceived_gender)\n",
    "\n",
    "viz_3 = subject_perceived_gender.plot(kind = 'bar', stacked = True,\n",
    "                                   title = 'People Pulled Over by Perceived Gender:\\n Arrested v Not Arrested',\n",
    "                                   color = ['blue', 'red','white'], alpha = .80, rot = 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Male subjects were arrest 31% of the arrests while female subjects were \n",
    "# arrested 28.65%. While the male subjects were stopped nearly 4 times the rate\n",
    "# of female, their arrest percentage was surprisingly close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "officer_gender = df.groupby(['Officer_Gender', 'Stop_Resolution']).Officer_Gender.count().unstack()\n",
    "officer_gender['% of Stops Ending In Arrest'] = (officer_gender['Yes'] / (officer_gender.sum(axis=1)))*100 \n",
    "\n",
    "print('Officer Gender\\n')\n",
    "print(officer_gender)\n",
    "\n",
    "viz_4 = officer_gender.plot(kind = 'bar', stacked = True,\n",
    "                                   title = 'Officers by Gender:\\n Arrested v Not Arrested',\n",
    "                                   color = ['blue', 'red','white'], alpha = .80, rot=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Male cops arrested individuals 31% of those pulled over, while male officers\n",
    "# arrested at a rate of 28%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "officer_race = df.groupby(['Officer_Race', 'Stop_Resolution']).Subject_Perceived_Race.count().unstack()\n",
    "officer_race['% of Stops Ending In Arrest'] = (officer_race['Yes'] / (officer_race.sum(axis=1)))*100 \n",
    "\n",
    "print('Officer Race\\n')\n",
    "print(officer_race)\n",
    "\n",
    "viz_5 = officer_race.plot(kind = 'bar', stacked = True,\n",
    "                                   title = 'Officers by Race:\\n Arrested v Not Arrested',\n",
    "                                   color = ['blue', 'red','white'], alpha = .80, rot=90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While white officers make up nearly 3 times the cops of all other races\n",
    "# combined, they were actually in the middle of the pack in terms of \n",
    "# arrests made. Officers of two + races, Hispanic/Latino, and Asian officers\n",
    "# were arresting at around ~25%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "am_or_pm = df.groupby(['Reported_AM_or_PM', 'Stop_Resolution']).Reported_AM_or_PM.count().unstack()\n",
    "am_or_pm['% of Stops Ending In Arrest'] = (am_or_pm['Yes'] / (am_or_pm.sum(axis=1)))*100 \n",
    "\n",
    "print('Reported AM or PM\\n')\n",
    "print(am_or_pm)\n",
    "\n",
    "viz_6 = am_or_pm.plot(kind = 'bar', stacked = True,\n",
    "                                   title = 'Time of Day (AM or PM):\\n Arrested v Not Arrested',\n",
    "                                   color = ['blue', 'red','white'], alpha = .80, rot=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are more Terry stops in the PM hours than AM, however the percentages\n",
    "# are less than 0.3% apart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_of_day = df.groupby(['Reported_Hour', 'Stop_Resolution']).Reported_Hour.count().unstack()\n",
    "hour_of_day['% of Stops Ending In Arrest'] = (hour_of_day['Yes'] / (hour_of_day.sum(axis=1)))*100 \n",
    "\n",
    "print('Reported Hour\\n')\n",
    "print(hour_of_day)\n",
    "\n",
    "viz_7 = hour_of_day.plot(kind = 'bar', stacked = True,\n",
    "                                   title = 'Hour of Day:\\n Arrested v Not Arrested',\n",
    "                                   color = ['blue', 'red','white'], alpha = .80, rot=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the data, arrests ebb and flow depending on time of day, \n",
    "# with peaks at midnight to 3am and 4pm to 7pm. However the highest\n",
    "# percentage of arrests made at 11am and 12pm, over 1 percentage point\n",
    "#away from the next two closest hours at 9am and 8pm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Model the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've cleaned and visualized the data, now let's fit it into\n",
    "# model. First starting with converting the target variable to binary,\n",
    "# OneHotEncoding, then normalizing the data with StandardScalar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Stop_Resolution'] = df['Stop_Resolution'].apply(lambda x: 0 if x=='No' else 1)\n",
    "\n",
    "df['Stop_Resolution'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "one_hot_df = pd.get_dummies(df)\n",
    "\n",
    "y = one_hot_df['Stop_Resolution']\n",
    "one_hot_df.drop('Stop_Resolution', axis=1, inplace=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_df, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_data_train = scaler.fit_transform(X_train)\n",
    "scaled_data_test = scaler.transform(X_test)\n",
    "\n",
    "scaled_df_train = pd.DataFrame(scaled_data_train, columns=one_hot_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the data is OneHotEncoded and normalized, let's fit a model, \n",
    "# starting with a Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(fit_intercept=False, C=1e12, solver='liblinear')\n",
    "logreg.fit(scaled_data_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_train = logreg.predict(scaled_data_train)\n",
    "y_hat_test = logreg.predict(scaled_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing Accuracy: ', accuracy_score(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the initial model, both the training and the test, both had an\n",
    "# accuracy of 57%. We can definitely make this better using more sophisticated\n",
    "# modeling. Next let's look at a confusion matrix to see where we can\n",
    "# see how the model is performing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(logreg, scaled_data_test, y_test,\n",
    "                     cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model is correctly predicting stops ending in no arrest 4896 times\n",
    "# while inccorectly predicting stops that were arrests as noa arrest 2163\n",
    "# times. \n",
    "\n",
    "# Next let's check the Precision, Recall, Accuracy, and F1 scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing Accuracy: ', accuracy_score(y_test, y_hat_test))\n",
    "print('Testing Precision: ', precision_score(y_test, y_hat_test))\n",
    "print('Testing Recall: ', recall_score(y_test, y_hat_test))\n",
    "print('Testing F1-Score: ', f1_score(y_test, y_hat_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The recall and F1-score ar absolutely abysmal. The accuracy is \n",
    "# slightly better than a coinflip which isn't much to write home about.\n",
    "# This time, we'll fit a K Nearest-Neighbors model to see if the data\n",
    "# works better in that context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier()\n",
    "clf.fit(scaled_data_train, y_train)\n",
    "y_hat_train = clf.predict(scaled_data_train)\n",
    "y_hat_test = clf.predict(scaled_data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing Accuracy: ', accuracy_score(y_test, y_hat_test))\n",
    "print('Testing Precision: ', precision_score(y_test, y_hat_test))\n",
    "print('Testing Recall: ', recall_score(y_test, y_hat_test))\n",
    "print('Testing F1-Score: ', f1_score(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(clf, scaled_data_test, y_test,\n",
    "                     cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The accuracy and precision went down\n",
    "# slightly in comparison to the Logistic Regression output.\n",
    "# A confusion matrix shows that it has only predicted '0', not\n",
    "# a great sign.\n",
    "# Since KNN has a default \"neighbors\" of 5, let's use GridSearchCV to \n",
    "# see if we can find a the best number of neighbors and run that\n",
    "# model one more time. \n",
    "\n",
    "# Source of following code: Eijaz Allibhai (https://towardsdatascience.com/building-a-k-nearest-neighbors-k-nn-model-with-scikit-learn-51209555453a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn2 = KNeighborsClassifier()\n",
    "param_grid = {'n_neighbors': np.arange(1,25)}\n",
    "knn_gscv = GridSearchCV(knn2, param_grid, cv=5)\n",
    "knn_gscv.fit(scaled_data_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_gscv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's rerun the KNN model to see exactly how this 24 is the best neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=24)\n",
    "clf.fit(scaled_data_train, y_train)\n",
    "y_hat_test = clf.predict(scaled_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing Accuracy: ', accuracy_score(y_test, y_hat_test))\n",
    "print('Testing Precision: ', precision_score(y_test, y_hat_test))\n",
    "print('Testing Recall: ', recall_score(y_test, y_hat_test))\n",
    "print('Testing F1-Score: ', f1_score(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy and precision are up but recall and are not great. \n",
    "# Let's move on to XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc = XGBClassifier()\n",
    "xgbc.fit(scaled_data_train, y_train)\n",
    "y_hat_test = xgbc.predict(scaled_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing Accuracy: ', accuracy_score(y_test, y_hat_test))\n",
    "print('Testing Precision: ', precision_score(y_test, y_hat_test))\n",
    "print('Testing Recall: ', recall_score(y_test, y_hat_test))\n",
    "print('Testing F1-Score: ', f1_score(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy and Precision are up, but everything else is worse. \n",
    "# Let's try tuning XGBoost using GridSearchCV\n",
    "# to see if we get better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'learning rate': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'max_depth': [6],\n",
    "    'min_child_weight': [1, 2, 3, 4, 5],\n",
    "    'subsample': [0.5, 0.7, 0.9],\n",
    "    'n_estimators': [100],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_clf = GridSearchCV(clf, param_grid, scoring='accuracy', cv=None, n_jobs=1)\n",
    "grid_clf.fit(scaled_data_train, y_train)\n",
    "\n",
    "best_parameters = grid_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc = XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1,\n",
    "                    min_child_weight=2, subsample=0.9)\n",
    "xgbc.fit(scaled_data_train, y_train)\n",
    "y_hat_test = xgbc.predict(scaled_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing Accuracy: ', accuracy_score(y_test, y_hat_test))\n",
    "print('Testing Precision: ', precision_score(y_test, y_hat_test))\n",
    "print('Testing Recall: ', recall_score(y_test, y_hat_test))\n",
    "print('Testing F1-Score: ', f1_score(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the highest our accuracy has been but barely. Was the 30\n",
    "# minutes of loading this GridSearch worth it? \n",
    "# Let's move to a decision tree and see if we can get better than \n",
    "# 69% accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=1\n",
    "dtc = DecisionTreeClassifier(criterion='entropy', random_state=SEED)\n",
    "dtc.fit(scaled_data_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = dtc.predict(scaled_data_test)\n",
    "\n",
    "print('Testing Accuracy: ', accuracy_score(y_test, y_hat_test))\n",
    "print('Testing Precision: ', precision_score(y_test, y_hat_test))\n",
    "print('Testing Recall: ', recall_score(y_test, y_hat_test))\n",
    "print('Testing F1-Score: ', f1_score(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=1\n",
    "dtc = DecisionTreeClassifier(criterion='gini', random_state=SEED)\n",
    "dtc.fit(scaled_data_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = dtc.predict(scaled_data_test)\n",
    "\n",
    "print('Testing Accuracy: ', accuracy_score(y_test, y_hat_test))\n",
    "print('Testing Precision: ', precision_score(y_test, y_hat_test))\n",
    "print('Testing Recall: ', recall_score(y_test, y_hat_test))\n",
    "print('Testing F1-Score: ', f1_score(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The difference between 'gini' and 'entropy' were basically non existent.\n",
    "# Let's try another GridSearch to see instead of manually changing parameters.\n",
    "# Hopefully we can find the best decision tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 2, 3, 4, 5],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 3, 4, 5]\n",
    "}\n",
    "\n",
    "dtc_grid = GridSearchCV(dtc, dt_param_grid, cv=3, return_train_score=True)\n",
    "\n",
    "dtc_grid.fit(scaled_data_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(criterion='gini', max_depth=2,\n",
    "                             min_samples_leaf=1, min_samples_split=2,\n",
    "                             random_state=SEED)\n",
    "dtc.fit(scaled_data_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_test = dtc.predict(scaled_data_test)\n",
    "\n",
    "print('Testing Accuracy: ', accuracy_score(y_test, y_hat_test))\n",
    "print('Testing Precision: ', precision_score(y_test, y_hat_test))\n",
    "print('Testing Recall: ', recall_score(y_test, y_hat_test))\n",
    "print('Testing F1-Score: ', f1_score(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It seems as though we were unable to get enough data for testing\n",
    "# metrics outside of Accuracy, probably deue to the low samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After running 7 models, the highest accuracy achieved was from an \n",
    "# XGBoost model with the best parameters found through GridSearchCV, \n",
    "# and even at that it was only able to be 69% accurate. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
